# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s6nAPKiTZOOsmjerPsL5WQ7pJvTDjJi2
"""

from google.colab import drive
drive.mount('/content/gdrive', force_remount = True)

import pandas as pd
import numpy as np
from matplotlib import pyplot
import seaborn as sns
from datetime import datetime
from math import sqrt
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from tensorflow.keras import optimizers
import plotly.offline as py
import plotly.graph_objs as go
from math import sqrt

"""### Data Processing"""

df = pd.read_csv('gdrive/My Drive/Big Data/Data/df.csv')
df = df.drop(['Unnamed: 0'], axis = 1)
values = df['last'].values.reshape(-1,1)
sentiment = df[['bitcoin_price_crypto', 'bitcoin_price', 'price_crypto', 'elon_musk_crypto',
             'cryptocurrency', 'dogecoin', 'Ethereum_ETH', 'Litecoin_LTC', 'blockchain']].values
values = values.astype('float32')
sentiment = sentiment.astype('float32')
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)


def create_dataset(dataset, look_back, sentiment, sent=False):
    '''
    look_back, which is the number of previous time steps to use 
    as input variables to predict the next time period 
    '''
    dataX, dataY = [], []
    for i in range(len(dataset) - look_back):
        if i >= look_back:
            a = dataset[i-look_back:i+1, 0]
            a = a.tolist()
            if(sent==True):
                for senti in sentiment[i].tolist():
                    a.append(senti)
            dataX.append(a)
            dataY.append(dataset[i + look_back, 0])
    return np.array(dataX), np.array(dataY)

"""### Modeling"""

def fit_LSTM(train, test, look_back, sentiment, sent, batch_size = 3, nb_epoch = 300, neurons = 1, dense = 100):
  # prepare data
  trainX, trainY = create_dataset(train, look_back, sentiment[0:train_size], sent)
  testX, testY = create_dataset(test, look_back, sentiment[train_size:len(scaled)], sent)
  trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
  testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
  
  # prepare model
  model = Sequential()
  model.add(LSTM(neurons, input_shape=(trainX.shape[1], trainX.shape[2])))
  model.add(Dense(dense, activation = 'relu'))
  model.add(Dense(1))
  # adam = optimizers.Adam(learning_rate=learning_rate, clipvalue=1.0)
  model.compile(loss='mean_squared_error', optimizer='adam')
  # history = model.fit(trainX, trainY, epochs=nb_epoch, batch_size=batch_size, 
  #                     validation_data=(testX, testY), verbose=0, shuffle=False)
  
  train_rmse, test_rmse = list(), list()
  for i in range(nb_epoch):
    model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)
    model.reset_states()
    
    # evaluate model on train data
    yhat = model.predict(trainX)
    yhat_inverse = scaler.inverse_transform(yhat.reshape(-1, 1))
    trainY_inverse = scaler.inverse_transform(trainY.reshape(-1, 1))
    rmse = sqrt(mean_squared_error(trainY_inverse, yhat_inverse))
    train_rmse.append(rmse)
    model.reset_states()
    
    # evaluate model on test data
    yhat = model.predict(testX)
    yhat_inverse = scaler.inverse_transform(yhat.reshape(-1, 1))
    testY_inverse = scaler.inverse_transform(testY.reshape(-1, 1))
    rmse = sqrt(mean_squared_error(testY_inverse, yhat_inverse))
    test_rmse.append(rmse)
    model.reset_states()
  
  history = pd.DataFrame()
  history['train'], history['test'] = train_rmse, test_rmse
  
  return history

"""A Baseline Model"""

train_size = int(len(scaled) * 0.8)
test_size = len(scaled) - train_size
train, test = scaled[0:train_size,:], scaled[train_size:len(scaled),:]
print(len(train), len(test))
split = train_size

look_back = 3
base_history = fit_LSTM(train, test, look_back, sentiment, False)

pyplot.plot(base_history['train'], color='blue')
pyplot.plot(base_history['test'], color='orange')
pyplot.savefig('gdrive/My Drive/Big Data/base_rmse.png')

print('TrainRMSE=%f, TestRMSE=%f' % (base_history['train'].iloc[-1], base_history['test'].iloc[-1]))



"""### Hyperparameter Tuning"""

def fit_LSTM(train, test, look_back, sentiment, sent, batch_size = 3, nb_epoch = 300, neurons = 1, dense = 100, learning_rate = 0.001):
  # prepare data
  trainX, trainY = create_dataset(train, look_back, sentiment[0:train_size], sent)
  testX, testY = create_dataset(test, look_back, sentiment[train_size:len(scaled)], sent)
  trainX = np.nan_to_num(trainX)
  testX = np.nan_to_num(testX)
  trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
  testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
  
  # prepare model
  model = Sequential()
  model.add(LSTM(neurons, input_shape=(trainX.shape[1], trainX.shape[2])))
  model.add(Dense(dense, activation = 'relu'))
  model.add(Dense(1))
  adam = optimizers.Adam(learning_rate=learning_rate, clipvalue=1.0)
  model.compile(loss='mean_squared_error', optimizer=adam)
  history = model.fit(trainX, trainY, epochs=nb_epoch, batch_size=batch_size, 
                      validation_data=(testX, testY), verbose=0, shuffle=False)
  
  # evaluate model on test data
  yhat = model.predict(testX)
  yhat_inverse = scaler.inverse_transform(yhat.reshape(-1, 1))
  testY_inverse = scaler.inverse_transform(testY.reshape(-1, 1))
  rmse = sqrt(mean_squared_error(testY_inverse, yhat_inverse))
  print('test rmse is %.3f', rmse)
  
  return history

"""#### All sentiment data added"""

look_back = 3
history_lb3_sent = fit_LSTM(train, test, look_back, sentiment, True)

history_lb3_sent.model.save('gdrive/My Drive/Big Data/ep300all')

print(history_lb3_sent.history['loss'][-1])

pyplot.plot(history_lb3_sent.history['loss'], color='blue')
pyplot.plot(history_lb3_sent.history['val_loss'], color='orange')
pyplot.savefig('gdrive/My Drive/Big Data/lb3_sent_all.png')

"""The rmse for test data went down first and then up. The min value corresponds to the right epoch."""

val_loss_list = list(history_lb3_sent.history['val_loss'])
print('The optimal epoch is', val_loss_list.index(min(val_loss_list)))

"""#### Epoch = 100 Try out different combinations of sentiment data..."""

look_back = 3
history_sent_comb = []
for i in range(3):
  history_sent_comb.append(fit_LSTM(train, test, look_back, sentiment[:, i*3:(i+1)*3], True, batch_size = 3, nb_epoch = 100))

history_sent_comb[0].model.save('gdrive/My Drive/Big Data/comb0')
history_sent_comb[1].model.save('gdrive/My Drive/Big Data/comb1')
history_sent_comb[2].model.save('gdrive/My Drive/Big Data/comb2')

trainX, trainY = create_dataset(train, look_back, sentiment[0:train_size], True)
testX, testY = create_dataset(test, look_back, sentiment[train_size:len(scaled)], True)
trainX = np.nan_to_num(trainX)
testX = np.nan_to_num(testX)
trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))

# plot for model with all sentiment data
model_all = fit_LSTM(train, test, look_back, sentiment, True, batch_size = 3, nb_epoch = 100)

model_all.model.save('gdrive/My Drive/Big Data/myModel_all')

yhat = model_all.model.predict(testX)
pyplot.plot(yhat, label='predict')
pyplot.plot(testY, label='true')
pyplot.legend()
pyplot.savefig('gdrive/My Drive/Big Data/lb3_sent_all_ep_100.png')

"""Try some other combinations..."""

sentiment_1 = df[['bitcoin_price_crypto', 'Ethereum_ETH', 'Litecoin_LTC', 'blockchain']].values
sentiment_1 = sentiment_1.astype('float32')

model = fit_LSTM(train, test, look_back, sentiment_1, True, batch_size = 3, nb_epoch = 100)

model.model.save('gdrive/My Drive/Big Data/comb3')

"""#### Look Back = 3, 5, 7, 10, 15, 20 [10 is selected]"""

temp = sentiment[:, 0:3]
trainX, trainY = create_dataset(train, look_back, temp[0:train_size], True)
testX, testY = create_dataset(test, look_back, temp[train_size:len(scaled)], True)
trainX = np.nan_to_num(trainX)
testX = np.nan_to_num(testX)
trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))

yhat = hist.model.predict(testX)
pyplot.plot(yhat, label='predict')
pyplot.plot(testY, label='true')
pyplot.legend()
pyplot.savefig('gdrive/My Drive/Big Data/lb3.png')

LookBack = [5,7,10]
for lb in LookBack:
  hist = fit_LSTM(train, test, look_back, sentiment[:, 0:3], True, batch_size = 3, nb_epoch = 100)
  hist.model.save('gdrive/My Drive/Big Data/lb'+str(lb))

for lb in LookBack:
  mymodel = keras.models.load_model('gdrive/My Drive/Big Data/lb'+str(lb))
  yhat = mymodel.predict(testX)
  pyplot.plot(yhat, label='predict')
  pyplot.plot(testY, label='true')
  pyplot.legend()
  pyplot.savefig('gdrive/My Drive/Big Data/lb'+str(lb)+'.png')
  pyplot.clf()



LookBack = [15, 20, 25]
for lb in LookBack:
  hist = fit_LSTM(train, test, look_back, sentiment[:, 0:3], True, batch_size = 3, nb_epoch = 100)
  hist.model.save('gdrive/My Drive/Big Data/lb'+str(lb))
  yhat = hist.model.predict(testX)
  pyplot.plot(yhat, label='predict')
  pyplot.plot(testY, label='true')
  pyplot.legend()
  pyplot.savefig('gdrive/My Drive/Big Data/lb'+str(lb)+'.png')
  pyplot.clf()

"""#### Learning rate = 0.1, 0.01, 0.001, 0.0001"""

Rate = [0.1, 0.01, 0.0001]
for r in Rate:
  hist = fit_LSTM(train, test, 10, sentiment[:, 0:3], True, batch_size = 3, nb_epoch = 100, learning_rate = r)
  hist.model.save('gdrive/My Drive/Big Data/rate'+str(r))
  yhat = hist.model.predict(testX)
  pyplot.plot(yhat, label='predict')
  pyplot.plot(testY, label='true')
  pyplot.legend()
  pyplot.savefig('gdrive/My Drive/Big Data/rate'+str(r)+'.png')
  pyplot.clf()

"""#### Dense layers"""

def fit_LSTM_adj(train, test, look_back, sentiment, sent, batch_size = 3, ):
  # prepare data
  trainX, trainY = create_dataset(train, look_back, sentiment[0:train_size], sent)
  testX, testY = create_dataset(test, look_back, sentiment[train_size:len(scaled)], sent)
  trainX = np.nan_to_num(trainX)
  testX = np.nan_to_num(testX)
  trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
  testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
  
  # prepare model
  model = Sequential()
  model.add(LSTM(neurons, input_shape=(trainX.shape[1], trainX.shape[2])))
  model.add(Dense(100, activation = 'relu'))
  model.add(Dense(100))
  model.add(Dense(1))
  adam = optimizers.Adam(learning_rate=0.01, clipvalue=1.0)
  model.compile(loss='mean_squared_error', optimizer=adam)
  history = model.fit(trainX, trainY, epochs=100, batch_size=3, 
                      validation_data=(testX, testY), verbose=0, shuffle=False)
  
  # evaluate model on test data
  yhat = model.predict(testX)
  yhat_inverse = scaler.inverse_transform(yhat.reshape(-1, 1))
  testY_inverse = scaler.inverse_transform(testY.reshape(-1, 1))
  rmse = sqrt(mean_squared_error(testY_inverse, yhat_inverse))
  print('test rmse is %.3f', rmse)
  
  return history

